# Kodeç³»ç»Ÿ Responses API æ”¯æŒé‡æ„æ–½å·¥æ–‡æ¡£

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

### ç›®æ ‡
å°†Kodeç³»ç»Ÿä»ç¡¬ç¼–ç çš„GPT-5æ£€æµ‹å‡çº§ä¸ºåŸºäºèƒ½åŠ›å£°æ˜çš„æ¨¡å‹ç³»ç»Ÿï¼Œæ”¯æŒæ‰€æœ‰Responses APIç±»æ¨¡å‹ï¼ˆGPT-5ã€GPT-6ã€GLM-5ç­‰ï¼‰ã€‚

### æ ¸å¿ƒåŸåˆ™
1. **é›¶ç ´åæ€§**: 100%ä¿ç•™ç°æœ‰åŠŸèƒ½
2. **æ¸è¿›å¼**: å¯éšæ—¶å›æ»š
3. **å¯æ‰©å±•**: æ–°æ¨¡å‹åªéœ€é…ç½®
4. **ä¼˜é›…æ€§**: æ¶ˆé™¤ç¡¬ç¼–ç ï¼Œç»Ÿä¸€å¤„ç†æµç¨‹

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ

### å½“å‰æ¶æ„ï¼ˆé—®é¢˜ï¼‰
```
ç”¨æˆ·è¾“å…¥ â†’ REPL â†’ query.ts â†’ queryLLM 
                                â†“
                         [ç¡¬ç¼–ç æ£€æµ‹]
                    if (isGPT5Model()) {...}
                    if (isGPT4Model()) {...}
                                â†“
                         ä¸åŒçš„APIè°ƒç”¨è·¯å¾„
```

### ç›®æ ‡æ¶æ„ï¼ˆè§£å†³æ–¹æ¡ˆï¼‰
```
ç”¨æˆ·è¾“å…¥ â†’ REPL â†’ query.ts â†’ queryLLM
                                â†“
                         [èƒ½åŠ›å£°æ˜ç³»ç»Ÿ]
                    ModelCapabilitiesæŸ¥è¯¢
                                â†“
                         [ç»Ÿä¸€é€‚é…å™¨]
                    ResponsesAPIAdapter / ChatCompletionsAdapter
                                â†“
                         ç»Ÿä¸€çš„APIè°ƒç”¨
```

## ğŸ“ æ–‡ä»¶ç»“æ„è§„åˆ’

```
src/
â”œâ”€â”€ types/
â”‚   â””â”€â”€ modelCapabilities.ts      # æ–°å»ºï¼šèƒ½åŠ›ç±»å‹å®šä¹‰
â”œâ”€â”€ constants/
â”‚   â””â”€â”€ modelCapabilities.ts      # æ–°å»ºï¼šæ¨¡å‹èƒ½åŠ›æ³¨å†Œè¡¨
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ adapters/                 # æ–°å»ºç›®å½•ï¼šé€‚é…å™¨
â”‚   â”‚   â”œâ”€â”€ base.ts              # æ–°å»ºï¼šåŸºç¡€é€‚é…å™¨ç±»
â”‚   â”‚   â”œâ”€â”€ responsesAPI.ts      # æ–°å»ºï¼šResponses APIé€‚é…å™¨
â”‚   â”‚   â””â”€â”€ chatCompletions.ts   # æ–°å»ºï¼šChat Completionsé€‚é…å™¨
â”‚   â”œâ”€â”€ modelAdapterFactory.ts   # æ–°å»ºï¼šé€‚é…å™¨å·¥å‚
â”‚   â”œâ”€â”€ claude.ts                 # ä¿®æ”¹ï¼šä½¿ç”¨æ–°ç³»ç»Ÿ
â”‚   â””â”€â”€ openai.ts                 # ä¿®æ”¹ï¼šæ¸…ç†ç¡¬ç¼–ç 
```

---

## ğŸš€ Phase 1: åŸºç¡€è®¾æ–½å»ºè®¾ï¼ˆç¬¬1-2å¤©ï¼‰

### ç›®æ ‡
åˆ›å»ºèƒ½åŠ›å£°æ˜ç³»ç»Ÿçš„åŸºç¡€æ¶æ„ï¼Œä¸å½±å“ç°æœ‰ä»£ç è¿è¡Œã€‚

### Step 1.1: åˆ›å»ºæ¨¡å‹èƒ½åŠ›ç±»å‹å®šä¹‰

**æ–‡ä»¶**: `src/types/modelCapabilities.ts` (æ–°å»º)

**ä»»åŠ¡**: å®šä¹‰æ¨¡å‹èƒ½åŠ›æ¥å£

```typescript
// å®Œæ•´ä»£ç  - ç›´æ¥å¤åˆ¶ç²˜è´´
export interface ModelCapabilities {
  // APIæ¶æ„ç±»å‹
  apiArchitecture: {
    primary: 'chat_completions' | 'responses_api'
    fallback?: 'chat_completions'  // Responses APIæ¨¡å‹å¯é™çº§
  }
  
  // å‚æ•°æ˜ å°„
  parameters: {
    maxTokensField: 'max_tokens' | 'max_completion_tokens'
    supportsReasoningEffort: boolean
    supportsVerbosity: boolean
    temperatureMode: 'flexible' | 'fixed_one' | 'restricted'
  }
  
  // å·¥å…·è°ƒç”¨èƒ½åŠ›
  toolCalling: {
    mode: 'none' | 'function_calling' | 'custom_tools'
    supportsFreeform: boolean
    supportsAllowedTools: boolean
    supportsParallelCalls: boolean
  }
  
  // çŠ¶æ€ç®¡ç†
  stateManagement: {
    supportsResponseId: boolean
    supportsConversationChaining: boolean
    supportsPreviousResponseId: boolean
  }
  
  // æµå¼æ”¯æŒ
  streaming: {
    supported: boolean
    includesUsage: boolean
  }
}

// ç»Ÿä¸€çš„è¯·æ±‚å‚æ•°
export interface UnifiedRequestParams {
  messages: any[]
  systemPrompt: string[]
  tools?: any[]
  maxTokens: number
  stream?: boolean
  previousResponseId?: string
  reasoningEffort?: 'minimal' | 'low' | 'medium' | 'high'
  verbosity?: 'low' | 'medium' | 'high'
  temperature?: number
}

// ç»Ÿä¸€çš„å“åº”æ ¼å¼
export interface UnifiedResponse {
  id: string
  content: string
  toolCalls?: any[]
  usage: {
    promptTokens: number
    completionTokens: number
    reasoningTokens?: number
  }
  responseId?: string  // ç”¨äºResponses APIçŠ¶æ€ç®¡ç†
}
```

### Step 1.2: åˆ›å»ºæ¨¡å‹èƒ½åŠ›æ³¨å†Œè¡¨

**æ–‡ä»¶**: `src/constants/modelCapabilities.ts` (æ–°å»º)

**ä»»åŠ¡**: ä¸ºæ‰€æœ‰æ¨¡å‹å®šä¹‰èƒ½åŠ›

```typescript
import { ModelCapabilities } from '../types/modelCapabilities'

// GPT-5çš„æ ‡å‡†èƒ½åŠ›å®šä¹‰
const GPT5_CAPABILITIES: ModelCapabilities = {
  apiArchitecture: {
    primary: 'responses_api',
    fallback: 'chat_completions'
  },
  parameters: {
    maxTokensField: 'max_completion_tokens',
    supportsReasoningEffort: true,
    supportsVerbosity: true,
    temperatureMode: 'fixed_one'
  },
  toolCalling: {
    mode: 'custom_tools',
    supportsFreeform: true,
    supportsAllowedTools: true,
    supportsParallelCalls: true
  },
  stateManagement: {
    supportsResponseId: true,
    supportsConversationChaining: true,
    supportsPreviousResponseId: true
  },
  streaming: {
    supported: false,  // Responses APIæš‚ä¸æ”¯æŒæµå¼
    includesUsage: true
  }
}

// Chat Completionsçš„æ ‡å‡†èƒ½åŠ›å®šä¹‰
const CHAT_COMPLETIONS_CAPABILITIES: ModelCapabilities = {
  apiArchitecture: {
    primary: 'chat_completions'
  },
  parameters: {
    maxTokensField: 'max_tokens',
    supportsReasoningEffort: false,
    supportsVerbosity: false,
    temperatureMode: 'flexible'
  },
  toolCalling: {
    mode: 'function_calling',
    supportsFreeform: false,
    supportsAllowedTools: false,
    supportsParallelCalls: true
  },
  stateManagement: {
    supportsResponseId: false,
    supportsConversationChaining: false,
    supportsPreviousResponseId: false
  },
  streaming: {
    supported: true,
    includesUsage: true
  }
}

// å®Œæ•´çš„æ¨¡å‹èƒ½åŠ›æ˜ å°„è¡¨
export const MODEL_CAPABILITIES_REGISTRY: Record<string, ModelCapabilities> = {
  // GPT-5ç³»åˆ—
  'gpt-5': GPT5_CAPABILITIES,
  'gpt-5-mini': GPT5_CAPABILITIES,
  'gpt-5-nano': GPT5_CAPABILITIES,
  'gpt-5-chat-latest': GPT5_CAPABILITIES,
  
  // GPT-4ç³»åˆ—
  'gpt-4o': CHAT_COMPLETIONS_CAPABILITIES,
  'gpt-4o-mini': CHAT_COMPLETIONS_CAPABILITIES,
  'gpt-4-turbo': CHAT_COMPLETIONS_CAPABILITIES,
  'gpt-4': CHAT_COMPLETIONS_CAPABILITIES,
  
  // Claudeç³»åˆ—ï¼ˆé€šè¿‡è½¬æ¢å±‚æ”¯æŒï¼‰
  'claude-3-5-sonnet-20241022': CHAT_COMPLETIONS_CAPABILITIES,
  'claude-3-5-haiku-20241022': CHAT_COMPLETIONS_CAPABILITIES,
  'claude-3-opus-20240229': CHAT_COMPLETIONS_CAPABILITIES,
  
  // O1ç³»åˆ—ï¼ˆç‰¹æ®Šçš„æ¨ç†æ¨¡å‹ï¼‰
  'o1': {
    ...CHAT_COMPLETIONS_CAPABILITIES,
    parameters: {
      ...CHAT_COMPLETIONS_CAPABILITIES.parameters,
      maxTokensField: 'max_completion_tokens',
      temperatureMode: 'fixed_one'
    }
  },
  'o1-mini': {
    ...CHAT_COMPLETIONS_CAPABILITIES,
    parameters: {
      ...CHAT_COMPLETIONS_CAPABILITIES.parameters,
      maxTokensField: 'max_completion_tokens',
      temperatureMode: 'fixed_one'
    }
  }
}

// æ™ºèƒ½æ¨æ–­æœªæ³¨å†Œæ¨¡å‹çš„èƒ½åŠ›
export function inferModelCapabilities(modelName: string): ModelCapabilities | null {
  if (!modelName) return null
  
  const lowerName = modelName.toLowerCase()
  
  // GPT-5ç³»åˆ—
  if (lowerName.includes('gpt-5') || lowerName.includes('gpt5')) {
    return GPT5_CAPABILITIES
  }
  
  // GPT-6ç³»åˆ—ï¼ˆé¢„ç•™ï¼‰
  if (lowerName.includes('gpt-6') || lowerName.includes('gpt6')) {
    return {
      ...GPT5_CAPABILITIES,
      streaming: { supported: true, includesUsage: true }
    }
  }
  
  // GLMç³»åˆ—
  if (lowerName.includes('glm-5') || lowerName.includes('glm5')) {
    return {
      ...GPT5_CAPABILITIES,
      toolCalling: {
        ...GPT5_CAPABILITIES.toolCalling,
        supportsAllowedTools: false  // GLMå¯èƒ½ä¸æ”¯æŒ
      }
    }
  }
  
  // O1ç³»åˆ—
  if (lowerName.startsWith('o1') || lowerName.includes('o1-')) {
    return {
      ...CHAT_COMPLETIONS_CAPABILITIES,
      parameters: {
        ...CHAT_COMPLETIONS_CAPABILITIES.parameters,
        maxTokensField: 'max_completion_tokens',
        temperatureMode: 'fixed_one'
      }
    }
  }
  
  // é»˜è®¤è¿”å›nullï¼Œè®©ç³»ç»Ÿä½¿ç”¨é»˜è®¤è¡Œä¸º
  return null
}

// è·å–æ¨¡å‹èƒ½åŠ›ï¼ˆå¸¦ç¼“å­˜ï¼‰
const capabilityCache = new Map<string, ModelCapabilities>()

export function getModelCapabilities(modelName: string): ModelCapabilities {
  // æ£€æŸ¥ç¼“å­˜
  if (capabilityCache.has(modelName)) {
    return capabilityCache.get(modelName)!
  }
  
  // æŸ¥æ‰¾æ³¨å†Œè¡¨
  if (MODEL_CAPABILITIES_REGISTRY[modelName]) {
    const capabilities = MODEL_CAPABILITIES_REGISTRY[modelName]
    capabilityCache.set(modelName, capabilities)
    return capabilities
  }
  
  // å°è¯•æ¨æ–­
  const inferred = inferModelCapabilities(modelName)
  if (inferred) {
    capabilityCache.set(modelName, inferred)
    return inferred
  }
  
  // é»˜è®¤ä¸ºChat Completions
  const defaultCapabilities = CHAT_COMPLETIONS_CAPABILITIES
  capabilityCache.set(modelName, defaultCapabilities)
  return defaultCapabilities
}
```

### Step 1.3: åˆ›å»ºåŸºç¡€é€‚é…å™¨ç±»

**æ–‡ä»¶**: `src/services/adapters/base.ts` (æ–°å»º)

**ä»»åŠ¡**: åˆ›å»ºadaptersç›®å½•å’ŒåŸºç¡€ç±»

```typescript
import { ModelCapabilities, UnifiedRequestParams, UnifiedResponse } from '../../types/modelCapabilities'
import { ModelProfile } from '../../utils/config'
import { Tool } from '../../Tool'

export abstract class ModelAPIAdapter {
  constructor(
    protected capabilities: ModelCapabilities,
    protected modelProfile: ModelProfile
  ) {}
  
  // å­ç±»å¿…é¡»å®ç°çš„æ–¹æ³•
  abstract createRequest(params: UnifiedRequestParams): any
  abstract parseResponse(response: any): UnifiedResponse
  abstract buildTools(tools: Tool[]): any
  
  // å…±äº«çš„å·¥å…·æ–¹æ³•
  protected getMaxTokensParam(): string {
    return this.capabilities.parameters.maxTokensField
  }
  
  protected getTemperature(): number {
    if (this.capabilities.parameters.temperatureMode === 'fixed_one') {
      return 1
    }
    if (this.capabilities.parameters.temperatureMode === 'restricted') {
      return Math.min(1, this.modelProfile.temperature || 0.7)
    }
    return this.modelProfile.temperature || 0.7
  }
  
  protected shouldIncludeReasoningEffort(): boolean {
    return this.capabilities.parameters.supportsReasoningEffort
  }
  
  protected shouldIncludeVerbosity(): boolean {
    return this.capabilities.parameters.supportsVerbosity
  }
}
```

### Step 1.4: åˆ›å»ºResponses APIé€‚é…å™¨

**æ–‡ä»¶**: `src/services/adapters/responsesAPI.ts` (æ–°å»º)

**ä»»åŠ¡**: å®ç°Responses APIé€‚é…å™¨

```typescript
import { ModelAPIAdapter } from './base'
import { UnifiedRequestParams, UnifiedResponse } from '../../types/modelCapabilities'
import { Tool } from '../../Tool'
import { zodToJsonSchema } from '../../utils/zodToJsonSchema'

export class ResponsesAPIAdapter extends ModelAPIAdapter {
  createRequest(params: UnifiedRequestParams): any {
    const { messages, systemPrompt, tools, maxTokens } = params
    
    // åˆ†ç¦»ç³»ç»Ÿæ¶ˆæ¯å’Œç”¨æˆ·æ¶ˆæ¯
    const systemMessages = messages.filter(m => m.role === 'system')
    const nonSystemMessages = messages.filter(m => m.role !== 'system')
    
    // æ„å»ºåŸºç¡€è¯·æ±‚
    const request: any = {
      model: this.modelProfile.modelName,
      input: this.convertMessagesToInput(nonSystemMessages),
      instructions: this.buildInstructions(systemPrompt, systemMessages)
    }
    
    // æ·»åŠ tokené™åˆ¶
    request[this.getMaxTokensParam()] = maxTokens
    
    // æ·»åŠ æ¸©åº¦ï¼ˆGPT-5åªæ”¯æŒ1ï¼‰
    if (this.getTemperature() === 1) {
      request.temperature = 1
    }
    
    // æ·»åŠ æ¨ç†æ§åˆ¶
    if (this.shouldIncludeReasoningEffort()) {
      request.reasoning = {
        effort: params.reasoningEffort || this.modelProfile.reasoningEffort || 'medium'
      }
    }
    
    // æ·»åŠ è¯¦ç»†åº¦æ§åˆ¶
    if (this.shouldIncludeVerbosity()) {
      request.text = {
        verbosity: params.verbosity || 'high'  // ç¼–ç ä»»åŠ¡é»˜è®¤é«˜è¯¦ç»†åº¦
      }
    }
    
    // æ·»åŠ å·¥å…·
    if (tools && tools.length > 0) {
      request.tools = this.buildTools(tools)
      
      // å¤„ç†allowed_tools
      if (params.allowedTools && this.capabilities.toolCalling.supportsAllowedTools) {
        request.tool_choice = {
          type: 'allowed_tools',
          mode: 'auto',
          tools: params.allowedTools
        }
      }
    }
    
    // æ·»åŠ çŠ¶æ€ç®¡ç†
    if (params.previousResponseId && this.capabilities.stateManagement.supportsPreviousResponseId) {
      request.previous_response_id = params.previousResponseId
    }
    
    return request
  }
  
  buildTools(tools: Tool[]): any[] {
    // å¦‚æœä¸æ”¯æŒfreeformï¼Œä½¿ç”¨ä¼ ç»Ÿæ ¼å¼
    if (!this.capabilities.toolCalling.supportsFreeform) {
      return tools.map(tool => ({
        type: 'function',
        function: {
          name: tool.name,
          description: tool.description || '',
          parameters: tool.inputJSONSchema || zodToJsonSchema(tool.inputSchema)
        }
      }))
    }
    
    // Custom toolsæ ¼å¼ï¼ˆGPT-5ç‰¹æ€§ï¼‰
    return tools.map(tool => {
      const hasSchema = tool.inputJSONSchema || tool.inputSchema
      const isCustom = !hasSchema || tool.freeformInput
      
      if (isCustom) {
        // Custom toolæ ¼å¼
        return {
          type: 'custom',
          name: tool.name,
          description: tool.description || ''
        }
      } else {
        // ä¼ ç»Ÿfunctionæ ¼å¼
        return {
          type: 'function',
          function: {
            name: tool.name,
            description: tool.description || '',
            parameters: tool.inputJSONSchema || zodToJsonSchema(tool.inputSchema)
          }
        }
      }
    })
  }
  
  parseResponse(response: any): UnifiedResponse {
    // å¤„ç†åŸºç¡€æ–‡æœ¬è¾“å‡º
    let content = response.output_text || ''
    
    // å¤„ç†ç»“æ„åŒ–è¾“å‡º
    if (response.output && Array.isArray(response.output)) {
      const messageItems = response.output.filter(item => item.type === 'message')
      if (messageItems.length > 0) {
        content = messageItems
          .map(item => {
            if (item.content && Array.isArray(item.content)) {
              return item.content
                .filter(c => c.type === 'text')
                .map(c => c.text)
                .join('\n')
            }
            return item.content || ''
          })
          .filter(Boolean)
          .join('\n\n')
      }
    }
    
    // è§£æå·¥å…·è°ƒç”¨
    const toolCalls = this.parseToolCalls(response)
    
    // æ„å»ºç»Ÿä¸€å“åº”
    return {
      id: response.id || `resp_${Date.now()}`,
      content,
      toolCalls,
      usage: {
        promptTokens: response.usage?.input_tokens || 0,
        completionTokens: response.usage?.output_tokens || 0,
        reasoningTokens: response.usage?.output_tokens_details?.reasoning_tokens
      },
      responseId: response.id  // ä¿å­˜ç”¨äºçŠ¶æ€ç®¡ç†
    }
  }
  
  private convertMessagesToInput(messages: any[]): any {
    // å°†æ¶ˆæ¯è½¬æ¢ä¸ºResponses APIçš„inputæ ¼å¼
    // å¯èƒ½éœ€è¦æ ¹æ®å®é™…APIè§„èŒƒè°ƒæ•´
    return messages
  }
  
  private buildInstructions(systemPrompt: string[], systemMessages: any[]): string {
    const systemContent = systemMessages.map(m => m.content).join('\n\n')
    const promptContent = systemPrompt.join('\n\n')
    return [systemContent, promptContent].filter(Boolean).join('\n\n')
  }
  
  private parseToolCalls(response: any): any[] {
    if (!response.output || !Array.isArray(response.output)) {
      return []
    }
    
    return response.output
      .filter(item => item.type === 'tool_call')
      .map(item => ({
        id: item.id || `tool_${Date.now()}`,
        type: 'tool_call',
        name: item.name,
        arguments: item.arguments  // å¯èƒ½æ˜¯æ–‡æœ¬æˆ–JSON
      }))
  }
}
```

### Step 1.5: åˆ›å»ºChat Completionsé€‚é…å™¨

**æ–‡ä»¶**: `src/services/adapters/chatCompletions.ts` (æ–°å»º)

**ä»»åŠ¡**: å®ç°Chat Completionsé€‚é…å™¨

```typescript
import { ModelAPIAdapter } from './base'
import { UnifiedRequestParams, UnifiedResponse } from '../../types/modelCapabilities'
import { Tool } from '../../Tool'
import { zodToJsonSchema } from '../../utils/zodToJsonSchema'

export class ChatCompletionsAdapter extends ModelAPIAdapter {
  createRequest(params: UnifiedRequestParams): any {
    const { messages, systemPrompt, tools, maxTokens, stream } = params
    
    // æ„å»ºå®Œæ•´æ¶ˆæ¯åˆ—è¡¨ï¼ˆåŒ…å«ç³»ç»Ÿæç¤ºï¼‰
    const fullMessages = this.buildMessages(systemPrompt, messages)
    
    // æ„å»ºè¯·æ±‚
    const request: any = {
      model: this.modelProfile.modelName,
      messages: fullMessages,
      [this.getMaxTokensParam()]: maxTokens,
      temperature: this.getTemperature()
    }
    
    // æ·»åŠ å·¥å…·
    if (tools && tools.length > 0) {
      request.tools = this.buildTools(tools)
      request.tool_choice = 'auto'
    }
    
    // æ·»åŠ æµå¼é€‰é¡¹
    if (stream) {
      request.stream = true
      request.stream_options = {
        include_usage: true
      }
    }
    
    // O1æ¨¡å‹çš„ç‰¹æ®Šå¤„ç†
    if (this.modelProfile.modelName.startsWith('o1')) {
      delete request.temperature  // O1ä¸æ”¯æŒtemperature
      delete request.stream  // O1ä¸æ”¯æŒæµå¼
      delete request.stream_options
    }
    
    return request
  }
  
  buildTools(tools: Tool[]): any[] {
    // Chat Completionsåªæ”¯æŒä¼ ç»Ÿçš„function calling
    return tools.map(tool => ({
      type: 'function',
      function: {
        name: tool.name,
        description: tool.description || '',
        parameters: tool.inputJSONSchema || zodToJsonSchema(tool.inputSchema)
      }
    }))
  }
  
  parseResponse(response: any): UnifiedResponse {
    const choice = response.choices?.[0]
    
    return {
      id: response.id || `chatcmpl_${Date.now()}`,
      content: choice?.message?.content || '',
      toolCalls: choice?.message?.tool_calls || [],
      usage: {
        promptTokens: response.usage?.prompt_tokens || 0,
        completionTokens: response.usage?.completion_tokens || 0
      }
    }
  }
  
  private buildMessages(systemPrompt: string[], messages: any[]): any[] {
    // åˆå¹¶ç³»ç»Ÿæç¤ºå’Œæ¶ˆæ¯
    const systemMessages = systemPrompt.map(prompt => ({
      role: 'system',
      content: prompt
    }))
    
    return [...systemMessages, ...messages]
  }
}
```

### Step 1.6: åˆ›å»ºé€‚é…å™¨å·¥å‚

**æ–‡ä»¶**: `src/services/modelAdapterFactory.ts` (æ–°å»º)

**ä»»åŠ¡**: åˆ›å»ºå·¥å‚ç±»æ¥é€‰æ‹©åˆé€‚çš„é€‚é…å™¨

```typescript
import { ModelAPIAdapter } from './adapters/base'
import { ResponsesAPIAdapter } from './adapters/responsesAPI'
import { ChatCompletionsAdapter } from './adapters/chatCompletions'
import { getModelCapabilities } from '../constants/modelCapabilities'
import { ModelProfile, getGlobalConfig } from '../utils/config'
import { ModelCapabilities } from '../types/modelCapabilities'

export class ModelAdapterFactory {
  /**
   * æ ¹æ®æ¨¡å‹é…ç½®åˆ›å»ºåˆé€‚çš„é€‚é…å™¨
   */
  static createAdapter(modelProfile: ModelProfile): ModelAPIAdapter {
    const capabilities = getModelCapabilities(modelProfile.modelName)
    
    // å†³å®šä½¿ç”¨å“ªç§API
    const apiType = this.determineAPIType(modelProfile, capabilities)
    
    // åˆ›å»ºå¯¹åº”çš„é€‚é…å™¨
    switch (apiType) {
      case 'responses_api':
        return new ResponsesAPIAdapter(capabilities, modelProfile)
      case 'chat_completions':
      default:
        return new ChatCompletionsAdapter(capabilities, modelProfile)
    }
  }
  
  /**
   * å†³å®šåº”è¯¥ä½¿ç”¨å“ªç§API
   */
  private static determineAPIType(
    modelProfile: ModelProfile,
    capabilities: ModelCapabilities
  ): 'responses_api' | 'chat_completions' {
    // å¦‚æœæ¨¡å‹ä¸æ”¯æŒResponses APIï¼Œç›´æ¥ä½¿ç”¨Chat Completions
    if (capabilities.apiArchitecture.primary !== 'responses_api') {
      return 'chat_completions'
    }
    
    // æ£€æŸ¥æ˜¯å¦æ˜¯å®˜æ–¹OpenAIç«¯ç‚¹
    const isOfficialOpenAI = !modelProfile.baseURL || 
      modelProfile.baseURL.includes('api.openai.com')
    
    // éå®˜æ–¹ç«¯ç‚¹ä½¿ç”¨Chat Completionsï¼ˆå³ä½¿æ¨¡å‹æ”¯æŒResponses APIï¼‰
    if (!isOfficialOpenAI) {
      // å¦‚æœæœ‰fallbacké€‰é¡¹ï¼Œä½¿ç”¨fallback
      if (capabilities.apiArchitecture.fallback === 'chat_completions') {
        return 'chat_completions'
      }
      // å¦åˆ™ä½¿ç”¨primaryï¼ˆå¯èƒ½ä¼šå¤±è´¥ï¼Œä½†è®©å®ƒå°è¯•ï¼‰
      return capabilities.apiArchitecture.primary
    }
    
    // æ£€æŸ¥æ˜¯å¦éœ€è¦æµå¼ï¼ˆResponses APIæš‚ä¸æ”¯æŒï¼‰
    const config = getGlobalConfig()
    if (config.stream && !capabilities.streaming.supported) {
      // éœ€è¦æµå¼ä½†Responses APIä¸æ”¯æŒï¼Œé™çº§åˆ°Chat Completions
      if (capabilities.apiArchitecture.fallback === 'chat_completions') {
        return 'chat_completions'
      }
    }
    
    // ä½¿ç”¨ä¸»è¦APIç±»å‹
    return capabilities.apiArchitecture.primary
  }
  
  /**
   * æ£€æŸ¥æ¨¡å‹æ˜¯å¦åº”è¯¥ä½¿ç”¨Responses API
   */
  static shouldUseResponsesAPI(modelProfile: ModelProfile): boolean {
    const capabilities = getModelCapabilities(modelProfile.modelName)
    const apiType = this.determineAPIType(modelProfile, capabilities)
    return apiType === 'responses_api'
  }
}
```

---

## ğŸ”„ Phase 2: é›†æˆä¸æµ‹è¯•ï¼ˆç¬¬3-4å¤©ï¼‰

### ç›®æ ‡
å°†æ–°ç³»ç»Ÿé›†æˆåˆ°ç°æœ‰ä»£ç ä¸­ï¼Œä¸æ—§ç³»ç»Ÿå¹¶è¡Œè¿è¡Œã€‚

### Step 2.1: ä¿®æ”¹claude.tsä½¿ç”¨æ–°ç³»ç»Ÿ

**æ–‡ä»¶**: `src/services/claude.ts` (ä¿®æ”¹)

**ä»»åŠ¡**: åœ¨queryLLMWithProfileä¸­æ·»åŠ æ–°çš„é€‚é…å™¨è·¯å¾„

**æ‰¾åˆ°å‡½æ•°**: `queryLLMWithProfile` (çº¦ç¬¬1182è¡Œ)

**ä¿®æ”¹å†…å®¹**:

```typescript
// åœ¨å‡½æ•°å¼€å¤´æ·»åŠ åŠŸèƒ½å¼€å…³
const USE_NEW_ADAPTER_SYSTEM = process.env.USE_NEW_ADAPTERS !== 'false'

// åœ¨è·å–modelProfileåæ·»åŠ æ–°è·¯å¾„
if (USE_NEW_ADAPTER_SYSTEM) {
  // ğŸš€ æ–°çš„é€‚é…å™¨ç³»ç»Ÿ
  const adapter = ModelAdapterFactory.createAdapter(modelProfile)
  
  // æ„å»ºç»Ÿä¸€è¯·æ±‚å‚æ•°
  const unifiedParams: UnifiedRequestParams = {
    messages: openaiMessages,  // ä½¿ç”¨å·²è½¬æ¢çš„OpenAIæ ¼å¼æ¶ˆæ¯
    systemPrompt: openaiSystem.map(s => s.content),
    tools: toolSchemas,
    maxTokens: getMaxTokensFromProfile(modelProfile),
    stream: config.stream,
    reasoningEffort: modelProfile.reasoningEffort,
    temperature: isGPT5Model(model) ? 1 : MAIN_QUERY_TEMPERATURE
  }
  
  // åˆ›å»ºè¯·æ±‚
  const request = adapter.createRequest(unifiedParams)
  
  // åˆ¤æ–­ä½¿ç”¨å“ªä¸ªAPIç«¯ç‚¹
  if (ModelAdapterFactory.shouldUseResponsesAPI(modelProfile)) {
    // è°ƒç”¨Responses APIï¼ˆå¤ç”¨ç°æœ‰çš„callGPT5ResponsesAPIï¼‰
    const response = await callGPT5ResponsesAPI(modelProfile, request, signal)
    return adapter.parseResponse(response)
  } else {
    // è°ƒç”¨Chat Completionsï¼ˆå¤ç”¨ç°æœ‰é€»è¾‘ï¼‰
    // ... ç°æœ‰çš„Chat Completionsè°ƒç”¨ä»£ç 
  }
} else {
  // ä¿ç•™åŸæœ‰é€»è¾‘å®Œå…¨ä¸å˜
  // ... ç°æœ‰çš„æ‰€æœ‰ä»£ç 
}
```

### Step 2.2: æ·»åŠ æµ‹è¯•è„šæœ¬

**æ–‡ä»¶**: `src/test/testAdapters.ts` (æ–°å»º)

**ä»»åŠ¡**: åˆ›å»ºæµ‹è¯•è„šæœ¬éªŒè¯æ–°ç³»ç»Ÿ

```typescript
import { ModelAdapterFactory } from '../services/modelAdapterFactory'
import { getGlobalConfig } from '../utils/config'

// æµ‹è¯•ä¸åŒæ¨¡å‹çš„é€‚é…å™¨é€‰æ‹©
const testModels = [
  { modelName: 'gpt-5', provider: 'openai' },
  { modelName: 'gpt-4o', provider: 'openai' },
  { modelName: 'claude-3-5-sonnet-20241022', provider: 'anthropic' },
  { modelName: 'o1', provider: 'openai' },
  { modelName: 'glm-5', provider: 'custom' }
]

testModels.forEach(model => {
  console.log(`Testing ${model.modelName}:`)
  const adapter = ModelAdapterFactory.createAdapter(model as any)
  console.log(`  Adapter type: ${adapter.constructor.name}`)
  console.log(`  Should use Responses API: ${ModelAdapterFactory.shouldUseResponsesAPI(model as any)}`)
})
```

### Step 2.3: æ¸…ç†ç¡¬ç¼–ç ï¼ˆå¯é€‰ï¼ŒPhase 3å†åšï¼‰

**æ–‡ä»¶**: `src/services/openai.ts` (ä¿®æ”¹)

**ä»»åŠ¡**: æ ‡è®°éœ€è¦ç§»é™¤çš„ç¡¬ç¼–ç éƒ¨åˆ†ï¼ˆå…ˆä¸åˆ é™¤ï¼‰

```typescript
// åœ¨isGPT5Modelå‡½æ•°ä¸Šæ·»åŠ æ³¨é‡Š
/**
 * @deprecated å°†è¢«ModelCapabilitiesç³»ç»Ÿæ›¿ä»£
 */
function isGPT5Model(modelName: string): boolean {
  return modelName.startsWith('gpt-5')
}
```

---

## ğŸš€ Phase 3: ä¼˜åŒ–ä¸æ¸…ç†ï¼ˆç¬¬5-6å¤©ï¼‰

### ç›®æ ‡
ç§»é™¤æ—§ä»£ç ï¼Œå®Œå…¨åˆ‡æ¢åˆ°æ–°ç³»ç»Ÿã€‚

### Step 3.1: ç§»é™¤åŠŸèƒ½å¼€å…³

**æ–‡ä»¶**: `src/services/claude.ts`

**ä»»åŠ¡**: ç§»é™¤USE_NEW_ADAPTER_SYSTEMæ£€æŸ¥ï¼Œé»˜è®¤ä½¿ç”¨æ–°ç³»ç»Ÿ

### Step 3.2: æ¸…ç†ç¡¬ç¼–ç å‡½æ•°

**æ–‡ä»¶åˆ—è¡¨**:
- `src/services/openai.ts` - ç§»é™¤isGPT5Modelå‡½æ•°
- `src/services/claude.ts` - ç§»é™¤isGPT5Modelå‡½æ•°  
- `src/services/openai.ts` - ç§»é™¤MODEL_FEATURESå¸¸é‡

### Step 3.3: æ›´æ–°æ–‡æ¡£

**æ–‡ä»¶**: `README.md`

**ä»»åŠ¡**: æ·»åŠ æ–°æ¨¡å‹æ”¯æŒè¯´æ˜

```markdown
## æ”¯æŒçš„æ¨¡å‹

æœ¬ç³»ç»Ÿé€šè¿‡èƒ½åŠ›å£°æ˜ç³»ç»Ÿæ”¯æŒä»¥ä¸‹APIç±»å‹ï¼š
- Chat Completions API: GPT-4, Claudeç­‰ä¼ ç»Ÿæ¨¡å‹
- Responses API: GPT-5, GPT-6, GLM-5ç­‰æ–°ä¸€ä»£æ¨¡å‹

æ·»åŠ æ–°æ¨¡å‹åªéœ€åœ¨ `src/constants/modelCapabilities.ts` ä¸­é…ç½®å³å¯ã€‚
```

---

## âœ… éªŒè¯æ¸…å•

### Phase 1å®Œæˆæ ‡å‡†
- [ ] æ‰€æœ‰æ–°æ–‡ä»¶åˆ›å»ºå®Œæˆ
- [ ] ä»£ç å¯ä»¥ç¼–è¯‘é€šè¿‡
- [ ] ç°æœ‰åŠŸèƒ½å®Œå…¨ä¸å—å½±å“

### Phase 2å®Œæˆæ ‡å‡†
- [ ] æ–°æ—§ç³»ç»Ÿå¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡åˆ‡æ¢
- [ ] GPT-5å¯ä»¥æ­£å¸¸ä½¿ç”¨
- [ ] æ‰€æœ‰ç°æœ‰æ¨¡å‹åŠŸèƒ½æ­£å¸¸

### Phase 3å®Œæˆæ ‡å‡†
- [ ] å®Œå…¨ä½¿ç”¨æ–°ç³»ç»Ÿ
- [ ] ä»£ç æ›´ç®€æ´æ¸…æ™°
- [ ] æ–°æ¨¡å‹å¯é€šè¿‡é…ç½®æ·»åŠ 

---

## ğŸ¯ å…³é”®æ³¨æ„äº‹é¡¹

1. **ä¸è¦åˆ é™¤ä»»ä½•ç°æœ‰åŠŸèƒ½ä»£ç **ï¼Œç›´åˆ°Phase 3
2. **å§‹ç»ˆä¿æŒå‘åå…¼å®¹**
3. **æ¯ä¸ªPhaseç»“æŸåéƒ½è¦æµ‹è¯•**
4. **å¦‚æœå‡ºç°é—®é¢˜å¯ä»¥ç«‹å³å›æ»š**

## ğŸ“ å¤–åŒ…ç¨‹åºå‘˜æ‰§è¡ŒæŒ‡å—

1. **ä¸¥æ ¼æŒ‰ç…§Phaseé¡ºåºæ‰§è¡Œ**ï¼Œä¸è¦è·³æ­¥
2. **å¤åˆ¶ç²˜è´´æä¾›çš„ä»£ç **ï¼Œä¸è¦è‡ªå·±ä¿®æ”¹
3. **é‡åˆ°é—®é¢˜ç«‹å³åœæ­¢å¹¶æŠ¥å‘Š**
4. **æ¯å®Œæˆä¸€ä¸ªStepéƒ½è¦git commit**ï¼Œæ–¹ä¾¿å›æ»š

---

æ­¤æ–‡æ¡£è®¾è®¡ä¸º"æ— è„‘æ‰§è¡Œ"çº§åˆ«ï¼Œå¤–åŒ…ç¨‹åºå‘˜åªéœ€è¦ï¼š
1. åˆ›å»ºæŒ‡å®šçš„æ–‡ä»¶
2. å¤åˆ¶ç²˜è´´æä¾›çš„ä»£ç 
3. åœ¨æŒ‡å®šä½ç½®ä¿®æ”¹ä»£ç 
4. è¿è¡Œæµ‹è¯•éªŒè¯

æ•´ä¸ªè¿‡ç¨‹ä¸éœ€è¦ç†è§£ä¸šåŠ¡é€»è¾‘ï¼Œåªéœ€è¦æœºæ¢°æ‰§è¡Œå³å¯ã€‚